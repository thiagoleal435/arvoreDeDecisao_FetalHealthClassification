1. Carregamento dos Dados
Primeiro, você precisará carregar seu arquivo CSV para um DataFrame pandas.

Python

    import pandas as pd

    # Supondo que seu arquivo CSV se chama 'cardiotocograma_data.csv'
    # Se o nome for diferente, substitua-o.
    try:
        df = pd.read_csv('cardiotocograma_data.csv')
        print("Dados carregados com sucesso!")
        print(df.head()) # Mostra as primeiras 5 linhas para verificar
    except FileNotFoundError:
        print("Erro: O arquivo 'cardiotocograma_data.csv' não foi encontrado. Por favor, verifique o nome e o caminho do arquivo.")

2. Separação de Features (X) e Variável Alvo (y)
Identifique suas features (características dos exames) e sua variável alvo (as classificações 'Normal', 'Suspeito' ou 'Patológico').

Python

    # Supondo que a coluna com as classificações seja 'classificacao' ou 'target'
    # e as demais colunas sejam as features.
    # Ajuste 'nome_da_coluna_alvo' para o nome real da sua coluna de classes.

    nome_da_coluna_alvo = 'NSP' # Exemplo, ajuste para o nome correto da sua coluna de classes

    X = df.drop(columns=[nome_da_coluna_alvo])
    y = df[nome_da_coluna_alvo]

    print("\nFeatures (X) - Exemplo:")
    print(X.head())
    print("\nVariável Alvo (y) - Exemplo:")
    print(y.head())

Observação Importante: As classes 'Normal', 'Suspeito' e 'Patológico' são categóricas. A maioria dos algoritmos de aprendizado de máquina, incluindo Árvores de Decisão no scikit-learn, funciona melhor com valores numéricos. Você pode mapear essas classes para números inteiros (por exemplo, Normal=0, Suspeito=1, Patológico=2) usando LabelEncoder ou manualmente.

Python

    from sklearn.2_preprocessing import LabelEncoder

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    print("\nClasses originais:", le.classes_)
    print("Classes mapeadas para números (y_encoded):")
    print(y_encoded[:5]) # Mostra os primeiros 5 valores codificados

3. Divisão dos Dados em Conjuntos de Treino e Teste
É crucial dividir seus dados para que você possa treinar o modelo em uma parte e testá-lo em dados que ele nunca viu antes, para ter uma avaliação imparcial de seu desempenho.

Python

    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)

    print(f"\nTamanho do conjunto de treino: {len(X_train)} registros")
    print(f"Tamanho do conjunto de teste: {len(X_test)} registros")

    test_size=0.3 significa que 30% dos dados serão usados para teste e 70% para treino. Você pode ajustar essa proporção.

    random_state=42 garante que a divisão seja a mesma toda vez que você executar o código, o que é útil para reprodutibilidade.

    stratify=y_encoded é muito importante para conjuntos de dados desbalanceados (onde uma classe tem muito mais exemplos que outra). Garante que a proporção de classes seja mantida nos conjuntos de treino e teste.

4. Criação e Treinamento da Árvore de Decisão
Agora, você instanciará e treinará seu modelo de Árvore de Decisão.

Python

    from sklearn.tree import DecisionTreeClassifier

    # Você pode ajustar hiperparâmetros como max_depth, min_samples_leaf, etc.
    # Um bom ponto de partida é deixar os padrões ou experimentar algumas opções.
    model = DecisionTreeClassifier(random_state=42)

    # Treinamento do modelo
    model.fit(X_train, y_train)

    print("\nModelo de Árvore de Decisão treinado com sucesso!")

5. Avaliação do Modelo
Após o treinamento, é hora de avaliar o quão bem seu modelo se comporta nos dados de teste. As métricas comuns para problemas de classificação multiclasse incluem acurácia, precisão, recall, F1-score e a matriz de confusão.

    Python

    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
    import matplotlib.pyplot as plt
    import seaborn as sns

    # Fazendo previsões no conjunto de teste
    y_pred = model.predict(X_test)

    # Acurácia
    accuracy = accuracy_score(y_test, y_pred)
    print(f"\nAcurácia do modelo: {accuracy:.4f}")

    # Relatório de Classificação
    # Mostra precisão, recall, F1-score para cada classe
    print("\nRelatório de Classificação:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    # Matriz de Confusão
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
    plt.xlabel('Previsto')
    plt.ylabel('Verdadeiro')
    plt.title('Matriz de Confusão')
    plt.show()

Considerações Adicionais para Melhorar o Modelo
Pré-processamento de Dados: Verifique se há valores ausentes ou outliers em suas features. Se houver, você pode precisar de estratégias de imputação ou remoção. Árvores de Decisão são relativamente robustas a diferentes escalas de features, mas normalização/padronização pode ser útil para outros modelos que você possa experimentar futuramente.

Ajuste de Hiperparâmetros (Hyperparameter Tuning): Os resultados da sua Árvore de Decisão podem ser melhorados ajustando seus hiperparâmetros (por exemplo, max_depth, min_samples_leaf, criterion). Você pode usar técnicas como Grid Search ou Randomized Search com Validação Cruzada para encontrar os melhores parâmetros.

Python

    from sklearn.model_selection import GridSearchCV

    param_grid = {
        'max_depth': [None, 5, 10, 15, 20],
        'min_samples_leaf': [1, 2, 4, 8],
        'criterion': ['gini', 'entropy']
    }

    grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    print("\nMelhores parâmetros encontrados:", grid_search.best_params_)
    print("Melhor acurácia com validação cruzada:", grid_search.best_score_)

    best_model = grid_search.best_estimator_
    y_pred_tuned = best_model.predict(X_test)
    print("\nAcurácia do modelo ajustado:", accuracy_score(y_test, y_pred_tuned))

Visualização da Árvore: Para entender como a Árvore de Decisão toma decisões, você pode visualizá-la (útil para max_depth menores).

Python

    from sklearn.tree import plot_tree

    plt.figure(figsize=(20, 15))
    plot_tree(model, feature_names=X.columns.tolist(), class_names=le.classes_, filled=True, rounded=True)
    plt.title("Árvore de Decisão (Visão Simplificada)")
    plt.show()
    Para árvores muito profundas, a visualização pode ficar confusa.